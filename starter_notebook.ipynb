{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08011cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac71c48",
   "metadata": {},
   "source": [
    "# 2023 Hackathon - Image Classification using CNN\n",
    "* <b> Author: </b> Hamzah Abdulrazzaq, Erika Hall, Shankara Abbineni \n",
    "* <b> Date: </b> September 22nd, 2023\n",
    "* <b> Objective: \n",
    "   </b> For this hackathon, we are focused on Convolutional Neural Networks (CNNs), with a special emphasis on a unique dataset sourced from NASA. We will be utilizing the \"Mars surface image (Curiosity rover) labeled data set\" which consists of 6691 images spanning 24 classes that were collected by the Mars Science Laboratory (MSL, Curosity) rover by three instruments (Mastcam Right eye, Mastcam Left eye, and MAHLI). The classes in the data set include wheel, drill, horizon, ground, rover rear deck, etc. The primary objective is to construct a CNN model that can accurately predict the classes of these images. Given that the dataset contains 24 classes, this is not an easy task. Criteria for winning submissions is determined by F1 score. The starter notebook provided here offers initial steps, helper functions, and documentation to help you succeed. While the notebook lays the groundwork, the onus is on participants to explore, innovate, and refine their models to meet the desired outcomes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b72d5b",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7730e6",
   "metadata": {},
   "source": [
    "- [Background Information: Convolutional Neural Nets](#Background-Information:-Convolutional-Neural-Nets)\n",
    "- [Importing Necessary Libraries](#Importing-Necessary-Libraries)\n",
    "- [Config Class + Basic Visualization](#Config-Class-+-Basic-Visualization)\n",
    "- [Data Imbalance](#Data-Imbalance)\n",
    "- [OpenCV](#OpenCV)\n",
    "- [Helper Functions](#Helper-Functions)\n",
    "- [Custom Image Dataset](#Custom-Image-Dataset)\n",
    "- [Image Augmentation](#Image-Augmentation)\n",
    "- [Size of Convolution Output](#Size-of-Convolution-Output)\n",
    "- [Visualize Loss and Accuracy](#Visualize-Loss-and-Accuracy)\n",
    "- [Test Model on Test Set](#Test-Model-on-Test-Set)\n",
    "- [Visualize Sample Predictions](#Visualize-Sample-Predictions)\n",
    "- [Submit Score](#Submit-Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9bbf0",
   "metadata": {},
   "source": [
    "#### For reference, <a href=\"resources.pdf\">here</a> are the preliminary training materials that were provided prior to the event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d529ca34",
   "metadata": {},
   "source": [
    "## Background Information: Convolutional Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff84253",
   "metadata": {},
   "source": [
    "A convolutional neural network (CNN or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They have proven to be very effective in image and video recognition, recommender systems, image classification, medical image analysis, natural language processing, brain-computer interfaces, and financial time series modeling. \n",
    "\n",
    "CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"fully-connectedness\" of these networks makes them prone to overfitting data. Typical ways of regularization include adding some form of magnitude measurement of weights to the loss function. CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns. Therefore, on the scale of connectedness and complexity, CNNs are on the lower extreme.\n",
    "\n",
    "Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field. (Wikipedia)\n",
    "\n",
    "Check out this [Interactive Demo for Image Kernels](https://deeplizard.com/resource/pavq7noze2) to see a visual demo on how different types of convolutional filters are applied to an image.\n",
    "\n",
    "<img src=\"https://ujwlkarn.files.wordpress.com/2016/07/convolution_schematic.gif?w=268&h=196\" style=\"width: 400px;\">\n",
    "<br>\n",
    "\n",
    "A typical CNN architecure consists of:\n",
    "- **Input Layer** – as you might expect, this layer contains the input image or sequence of images. In this layer, the number of neurons equals the number of features (which is the number of pixels in image data). \n",
    "- **Convolutional Layer(s)** – this layer applies filters or ‘kernels’ to your image. These filters are smaller than your image size. A filter crosses your image in ‘strides’, and after each stride the model returns a value denoting how closely the portion of the image matches the filter.  [What are Convolutional Neural Networks (CNN)?](https://www.bing.com/videos/search?q=convolutional+neural+network&docid=603549927117705930&mid=E7ED04059A9474B24BFEE7ED04059A9474B24BFE&view=detail&FORM=VIRE) is a short video that gives a great visual illustration of this concept. \n",
    "- **Pooling Layer(s)** – this layer’s job is to reduce the size, which makes the model faster and combats overfitting. This layer summarizes each filter-sized portion of the image. In other words, for a group of features or pixels, the pooling layer returns a single feature that represents the group. Two common approaches to pooling are (1) average pooling, where the layer returns an average of the features and (2) max pooling, where the layer returns the maximum-valued feature. Geeks for Geeks goes into further detail on how pooling layers work and how to implement them in [CNN | Introduction to Pooling Layer](https://www.geeksforgeeks.org/cnn-introduction-to-pooling-layer/). \n",
    "- **Fully Connected Layer(s)** – this layer computes the final task. \n",
    "- **Output Layer** – this layer takes the final calculation and converts it to a probability score for each class.   \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://lh3.googleusercontent.com/yrHzday2CwSYLkXf9yKSoH-BpjqnnAuyiMvPAS5yS3-lFnl5jwkR6FoT_v2Vbi14s414fJSORuGLRQbHyYp6dtHDItRcSQnRWcd1JRGbZC5VlGTvH80gFZrHw8qg2Tx7ca2HYKFc\" style=\"width: 600px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e571e",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea53435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch   \n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import transforms\n",
    "from torch.nn import Module, Conv2d, ConvTranspose2d, Dropout2d, BatchNorm2d\n",
    "from torchvision.datasets import ImageFolder\n",
    "from IPython.display import display, Math, Latex\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "\n",
    "# plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f069c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87391da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_raw = pd.read_csv('domino/datasets/UPRM_Hackathon_2023/train_data_uprm.csv') # read in training data\n",
    "val_data_raw = pd.read_csv('domino/datasets/UPRM_Hackathon_2023/val_data_uprm.csv') # read in validation data\n",
    "test_data_raw = pd.read_csv('domino/datasets/UPRM_Hackathon_2023/test_data_uprm.csv') # read in testing data.  \n",
    "# reported score will be based on model performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d5434e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb95f8",
   "metadata": {},
   "source": [
    "## Config Class + Basic Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0893f27a",
   "metadata": {},
   "source": [
    "#### Note: The config class is often used to centralize various settings and parameters. For our purposes, we will be using it to make sure all of our configuration settings are in one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d2dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config: \n",
    "    img_dimensions = (128, 128)\n",
    "    channels = 3 # number of channels in each image (RGB)\n",
    "    images_path = r\"C:\\Users\\red78\\dev\\lm\\domino\\datasets\\UPRM_Hackathon_2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79dfd109",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object is not subscriptable  C:\\Users\\red78\\dev\\lm\\domino\\datasets\\UPRM_Hackathon_2023\\0073MR0003940000103660E01_DRCL.JPG\n"
     ]
    }
   ],
   "source": [
    "def img_display(path, label):\n",
    "    try:\n",
    "        read_image = cv2.imread(path)\n",
    "        read_image = read_image[:,:,::-1]\n",
    "        plt.imshow(read_image)\n",
    "        plt.grid(False)\n",
    "        plt.title(f'class: {label}')\n",
    "        plt.show()\n",
    "        print(path)\n",
    "    except Exception as e:\n",
    "        print(e,path)\n",
    "\n",
    "\n",
    "# img_display(\"C:\\\\Users\\\\red78\\\\dev\\\\lm\\\\c130_hercules.jpg\",'hi')\n",
    "img_display(\" C:\\\\Users\\\\red78\\\\dev\\\\lm\\\\domino\\\\datasets\\\\UPRM_Hackathon_2023\\\\0073MR0003940000103660E01_DRCL.JPG\",'hi')\n",
    "\n",
    "train_images = [Config.images_path + f'\\{i}' for i in train_data_raw['image']]\n",
    "train_labels = [i for i in train_data_raw['label']]\n",
    "\n",
    "val_images = [os.path.join(Config.images_path, i) for i in val_data_raw['image']]\n",
    "val_labels = [i for i in val_data_raw['label']]\n",
    "\n",
    "test_images = [os.path.join(Config.images_path, i) for i in test_data_raw['image']]\n",
    "test_labels = [i for i in test_data_raw['label']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(len(train_images)):\n",
    "#     img_display(train_images[i],train_labels[i])\n",
    "# img_display(train_images[3], train_labels[3])\n",
    "# img_display(val_images[5], val_labels[5])\n",
    "# img_display(test_images[3], test_labels[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7347b0cf",
   "metadata": {},
   "source": [
    "## Data Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1089047",
   "metadata": {},
   "source": [
    "#### $\\hspace{3mm}$ i. Defintion\n",
    "\n",
    "$\\hspace{6mm}$ An imbalanced dataset is simply a dataset where the class proportions are skewed. Below is a plot showing the data imbalance in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c88f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(15,5))\n",
    "train_data_raw['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f04234",
   "metadata": {},
   "source": [
    "#### $\\hspace{3mm}$ ii. Strategies for handling it \n",
    "\n",
    "$\\hspace{6mm}$There are quite a few approaches to choose from to combat imbalanced datasets: \n",
    "> 1. Oversampling and undersampling\n",
    "3. Image augmentation\n",
    "4. Including weights for each class in a loss function\n",
    "\n",
    "$\\hspace{12mm}$and many more!\n",
    "\n",
    " \n",
    "\n",
    "$\\hspace{6mm}$ You can refer to the pretraining materials for details on how to implement these strategies!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0a38aa",
   "metadata": {},
   "source": [
    "# OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c9d365",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### $\\hspace{3mm}$ i. Definition\n",
    "\n",
    "> OpenCV is an open source library for C++, Python, and Java that can be used to process images or videos. In our case, we will use OpenCV to prepare a set of images for model ingestion. [OpenCV – Overview](https://www.geeksforgeeks.org/opencv-overview/) has lots more detail on the specific capabilities of OpenCV if you’re interested in delving deeper into this library.\n",
    "\n",
    " \n",
    "\n",
    "#### $\\hspace{3mm}$ ii. Important operations\n",
    "> In this workbook, the most important OpenCV operations will be the ***basic image operations*** and the ***image augmentation operations***. Feel free to refer to your pretraining materials for details and examples demonstrating how to implement them!\n",
    "\n",
    "<a href=\"open_cv_exploration_example.ipynb\">This notebook</a> implements some OpenCV functions on our current dataset. Feel free to pull from these image augmentation examples to experiment with other image augmentation technqies!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f9319",
   "metadata": {},
   "source": [
    "## **Note**\n",
    "\n",
    "> if you would like to apply a particular OpenCV function to your input data, please do so inside the <code>resize_normalize</code> function below. A commented out example is available for reference inside the function. The <code>resize_normalize</code> function is then used inside the <code>CustomImageDataset</code> class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d85a558",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7325f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_normalize(img):\n",
    "    \"\"\"\n",
    "    Argument: image\n",
    "    steps: 1. Resize image to specified dimensions\n",
    "           2. transpose so that channels are first\n",
    "           3. resize image\n",
    "    \"\"\"\n",
    "    img = cv2.resize(img, Config.img_dimensions)\n",
    "    \n",
    "    #ret,img = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n",
    "    img = np.transpose(img, (2,0,1)) # reorder it to channels first\n",
    "    img = torch.from_numpy(img).float()\n",
    "    img = img / 255\n",
    "    \n",
    "    \n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392eec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(x):\n",
    "    return x.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866aa63f",
   "metadata": {},
   "source": [
    "### Balance the data using oversampling + undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COUNT = 250 #target 250 samples per class\n",
    "\n",
    "def balance_data(df, target_count):\n",
    "    \"\"\" Arguments: 1. df to balance\n",
    "                   2. desired count in each class\n",
    "        Steps: 1. Separate the different classes in the dataframe\n",
    "               2. loop through each class \n",
    "                   a. if its a majority class -> undersample\n",
    "                   b. if its a minority class -> oversample\n",
    "               3. concat into one dataframe\n",
    "               \n",
    "    \"\"\"\n",
    "    \n",
    "    ## TODO\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a248db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw = balance_data(train_data_raw, target_count=TARGET_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9206862d",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b15bb61",
   "metadata": {},
   "source": [
    "## Custom Image Dataset \n",
    "\n",
    "### Using the Dataset primitive to create a custom image dataset \n",
    "\n",
    "We created our own custom image dataset for this Hackathon, using images from a NASA post on [data.gov](https://www.geeksforgeeks.org/opencv-overview/). You can also find the image files on [Zenodo](https://zenodo.org/record/1049137). \n",
    "\n",
    "For a better understanding of the <code>CustomImageDataset</code> code provided, refer to the pretraining materials or check out [the official documetation](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) on *how* to create your own image dataset using pytorch.\n",
    "\n",
    "##### ** Note: A basic Custom Image Dataset is set up below for you to use, but feel free to experiment and create your own! **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dataset, img_dir, transform = None, target_transform = None):\n",
    "        valid_rows = []\n",
    "        for idx, row in dataset.iterrows():\n",
    "            img_path = os.path.join(img_dir, row['image'])\n",
    "            if os.path.exists(img_path):\n",
    "                valid_rows.append(row)\n",
    "            else:\n",
    "                print(f\"File not found: {img_path}\")\n",
    "\n",
    "        self.dataset = pd.DataFrame(valid_rows).reset_index(drop=True)  # Reset index in order to not mismatch\n",
    "        self.image_labels = list(dataset['label'])\n",
    "        self.image_path = list(dataset['image'])\n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.dataset.loc[idx, 'image'])\n",
    "        image = cv2.imread(img_path)\n",
    "        label = self.dataset.loc[idx, 'label'] \n",
    "        image = resize_normalize(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439583e",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "\n",
    "\n",
    "The <code>transforms</code> function can be used to augment images for better training results. \n",
    "Some augmentation methods that can be passed to <code>transforms</code> include horizontal flip, vertical flip, random crop, etc. Official documentation can be found [here](https://pytorch.org/vision/stable/transforms.html), and deeper explanation can be found in the pretraining resources.\n",
    "\n",
    "<a href=\"data_augmentation_example.ipynb\"> Here</a> is a workbook that provides examples of how to use <code>transforms</code> to augment our image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bbf1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "])\n",
    "\n",
    "\n",
    "train_ds = CustomImageDataset(train_data_raw, Config.images_path, transform=transform)\n",
    "valid_ds = CustomImageDataset(val_data_raw, Config.images_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591b419",
   "metadata": {},
   "source": [
    "## Size of Convolution Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda92098",
   "metadata": {},
   "source": [
    "Each layer of our CNN Model below yields a uniquely-sized output. If you would like to see how to use this equation to calculate a layer's output size, refer to the pretraining resources: \n",
    "\n",
    "$$ OutputSize = \\lfloor{\\frac{n+2(padding)-kernel}{stride}}\\rfloor + 1 $$\n",
    "\n",
    "If you alter the model's architecture, this equation may be helpful for understanding how your new model works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a265d464",
   "metadata": {},
   "source": [
    "### Note: Below is a basic CNN architecture to help you get started. You will need add additional layers and customizations to get the desired model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08acb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.drop = Dropout2d(0.3)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size = 3, padding = 1) # stride set to 1 by default\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 3, padding = 1)\n",
    "        self.fc1 = nn.Linear(65536, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        # outputsize = 32x128x128. -> calculation: output_size = [(128+2(1)-3)/1] + 1 = 128\n",
    "        x = nn.Sigmoid()(x)\n",
    "        x = F.max_pool2d(x, kernel_size = 2, stride = 2) # outputsize = 32x64x64\n",
    "        \n",
    "        \n",
    "        x = self.conv2(x)# outputsize = 64x64x64\n",
    "        x = nn.Sigmoid()(x)\n",
    "        x = F.max_pool2d(x, 2, 2) # outputsize = 64x32x32\n",
    "    \n",
    "        \n",
    "        x = x.view(x.size(0), -1) # flatten output -> shape: (64*32*32,1)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bd4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9838a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    CNN_model, train_ds, valid_ds, epochs, learning_rate, num_classes\n",
    "):\n",
    "    torch.manual_seed(39) # set seed\n",
    "    model = CNN_model(num_classes) # instantiate model\n",
    "    print(model) # print model architecture\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds, batch_size=32, shuffle=True\n",
    "    ) # this will allow us to iterate through images in batches\n",
    "    test_dataloader = DataLoader(\n",
    "        valid_ds, batch_size=32, shuffle=False\n",
    "    )  \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # value of 0 would denote perfect model. It is also a measure of class purity. \n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=1e-4\n",
    "    ) # In short, optimizer is used to minimize loss. We are using Adam since it tends\n",
    "    # to have smaller variance and faster convergence.\n",
    "\n",
    "    acc_hist_train = []\n",
    "    acc_hist_val = []\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    \n",
    "    early_stopper = EarlyStopper(patience=5, min_delta=10)\n",
    "    for epoch in range(1, epochs + 1): # loop over dataset many times\n",
    "        #print(f'memory cleared: {torch.cuda.memory_allocated() / (1024**2):.2f} allocated')\n",
    "        #torch.cuda.empty_cache()\n",
    "        model.train()  # set to train mode\n",
    "        running_loss = 0.0\n",
    "        accuracy_hist_train = 0\n",
    "        for x_batch, y_batch in tqdm(train_dataloader):\n",
    "            pred = model(x_batch)\n",
    "            loss = criterion(pred, y_batch)\n",
    "            loss.backward() # back propoagation\n",
    "            optimizer.step() # updates weights\n",
    "            optimizer.zero_grad() # zero out the gradients \n",
    "            running_loss += loss.item()\n",
    "            is_correct = (torch.argmax(pred, dim=1) == y_batch).sum().float()\n",
    "\n",
    "            \n",
    "            # how many of predictions were correct?\n",
    "            accuracy_hist_train += is_correct\n",
    "            \n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "        accuracy_hist_train /= len(train_dataloader.dataset)\n",
    "        running_loss /= len(train_dataloader.dataset)\n",
    "        # since we accu\n",
    "        acc_hist_train.append(accuracy_hist_train)\n",
    "        train_losses.append(running_loss)\n",
    "\n",
    "        accuracy_hist_valid = 0\n",
    "        model.eval() # disables dropout, etc\n",
    "        with torch.no_grad(): # disables gradient \n",
    "            val_loss = 0.0\n",
    "            running_f1_score = 0.0\n",
    "            for x, y in test_dataloader:\n",
    "                pred = model(x) # predict \n",
    "                is_correct = (torch.argmax(pred, dim=1) == y).sum().float()\n",
    "                loss = criterion(pred, y)\n",
    "                val_loss += loss\n",
    "                f1_score_calc = f1_score(to_numpy(torch.argmax(pred, dim=1)), to_numpy(y), average='macro')\n",
    "                running_f1_score += f1_score_calc\n",
    "                accuracy_hist_valid += is_correct\n",
    "            accuracy_hist_valid /= len(test_dataloader.dataset)\n",
    "            val_loss /= len(test_dataloader.dataset)\n",
    "            acc_hist_val.append(accuracy_hist_valid)\n",
    "            running_f1_score /= np.ceil(len(val_data_raw)/32)\n",
    "\n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            print(f'Early Stop -> Epoch = {epoch}')\n",
    "            break\n",
    "        \n",
    "        if (epoch > 1 and val_loss < np.min(val_losses)) or epoch == 1:\n",
    "            torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        \n",
    "        val_losses.append(to_numpy(val_loss))\n",
    "        \n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(\"Train Acc:\\t{:.3f}\".format(accuracy_hist_train))\n",
    "        print(\"Val Acc:\\t{:.3f}\".format(accuracy_hist_valid))\n",
    "        print(\"Train Loss: \\t{:.3f}\".format(running_loss))\n",
    "        print(\"Val Loss: \\t{:.3f}\".format(val_loss))\n",
    "        print(\"Val F1 Score: \\t{:.3f}\".format(running_f1_score))\n",
    "        print(\"---------------------------------------------\")\n",
    "        \n",
    "    model = CNN_model(num_classes)\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    model.eval()\n",
    "\n",
    "    return model, acc_hist_train, acc_hist_val, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac28719",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "model, acc_hist_train, acc_hist_val, train_losses, val_losses = train_model(CNN_model, train_ds, valid_ds, \n",
    "                                                  num_epochs, learning_rate=0.001, num_classes=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e94196",
   "metadata": {},
   "source": [
    "## Visualize Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb267d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_axis = np.arange(num_epochs) + 1\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(x_axis, train_losses, '-o', label = 'Train loss')\n",
    "ax.plot(x_axis, val_losses, '--<', label = 'Validation loss')\n",
    "ax.legend(fontsize=15)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x_axis, acc_hist_train, '-o', label='Train Accuracy')\n",
    "ax.plot(x_axis, acc_hist_val, '--<', label='Validation Accuracy')        \n",
    "ax.legend(fontsize=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.set_ylabel('Epoch', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c113a8",
   "metadata": {},
   "source": [
    "## Test Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd610af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_ds = CustomImageDataset(test_data_raw, Config.images_path)\n",
    "test_dl = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "accuracy_test = 0.0\n",
    "actual_labels = []\n",
    "predicted_labels = []\n",
    "with torch.no_grad():\n",
    "    # disables gradient \n",
    "    val_loss = 0.0\n",
    "    for x, y in test_dl:\n",
    "        pred = model(x) #predict \n",
    "        is_correct = (torch.argmax(pred, dim=1) == y).sum().float()\n",
    "        accuracy_test += is_correct.mean()\n",
    "        actual_labels.append(to_numpy(y))\n",
    "        predicted_labels.append(to_numpy(torch.argmax(pred, dim=1)))\n",
    "    accuracy_test /= len(test_dl.dataset) \n",
    "    \n",
    "print(f'test accuracy: {accuracy_test}')\n",
    "print(f\"{bcolors.OKCYAN}{bcolors.BOLD}NOTE: Test Accuracy above is provided for reference only. It can be a misleading statistic for imbalanced data.\")\n",
    "predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "actual_labels = [item for sublist in actual_labels for item in sublist]\n",
    "F1_score = f1_score(actual_labels, predicted_labels, average='macro')\n",
    "print(f'f1_score: {F1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b761559a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(actual_labels, predicted_labels)\n",
    "\n",
    "labels = list(set(test_data_raw['label']))\n",
    "\n",
    "fig = plt.figure(figsize=(16, 14))\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel(\"Predicted\", fontsize=20)\n",
    "ax.xaxis.set_label_position(\"bottom\")\n",
    "plt.xticks(rotation=90)\n",
    "ax.xaxis.tick_bottom()\n",
    "\n",
    "ax.set_ylabel(\"True\", fontsize=20)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.title(\"Refined Confusion Matrix\", fontsize=20)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "F1_score = f1_score(actual_labels, predicted_labels, average='macro')\n",
    "\n",
    "print(f'f1_score: {F1_score}')\n",
    "print(f\"{bcolors.FAIL}{bcolors.BOLD}{bcolors.UNDERLINE}Recall: Criteria for winning submissions is determined by the F1 score above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c763584",
   "metadata": {},
   "source": [
    "### REMARK:  Confusion Matrix provided below is a great way to gauge where the model is underperforming. \n",
    ">  A confusion matrix is a great way to graphically visualize True Positives, False Positives, True Negatives, and False Negatives. For Example, the confusion matrix can demonstrate to you that your model is adept at distinguishing between wheel and rover rear deck, but not inept when it comes to distinguishing between portion tube and portion tube opening. \n",
    "> Several strategies you can consider based on feedback received from confusion matrix: \n",
    " >> 1. Modify CNN architecture? \n",
    " >> 2. Focus on Hyperparameter tuning? \n",
    " >> 3. Visualize sample images for portion tube and portion tube opening and experiment with opencv functions that can help better distinguish between the two classes as a precossessing steps? \n",
    " \n",
    " Above are a few suggestions for how you can tackle such a problem. Please feel free to research and experiment on your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff939f4f",
   "metadata": {},
   "source": [
    "## Visualize Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2688261",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(test_dl)) # sample images, labels from dataset\n",
    "preds = torch.argmax(model(image), dim=1) # get predictions for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41d638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we plot the image, its prediction, and its true label to investigate our model performance\n",
    "plt.figure(figsize=(10,10))\n",
    "j = 0\n",
    "for img, lbl, predicted in zip(image[:12,:,:,:], label[:12], preds[:12]):\n",
    "    img = to_numpy(img)\n",
    "    lbl = to_numpy(lbl)\n",
    "    img = np.transpose(img, (1,2,0))\n",
    "    plt.subplot(4, 3, j+1)\n",
    "    plt.imshow(img[:,:,::-1])\n",
    "    plt.tight_layout()\n",
    "    plt.title(f'pred: {predicted}, actual: {lbl}')\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05880b5",
   "metadata": {},
   "source": [
    "# Grading/Submission Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417a09f5",
   "metadata": {},
   "source": [
    "## Submissions\n",
    "Please email your submissions to jean.han@lmco.com. All these items must be complete in order for your submission to be considered for evaluation. \n",
    "> 1. Email title set to *yourteamname_submission* \n",
    "> 2. The python notebook with all your code (.ipynb) \n",
    "> 3. F2 Score copied in with all the digits\n",
    "\n",
    "Feel free to check out these resources to learn more about evaluation metrics used in Data Science: \n",
    "> - https://towardsdatascience.com/how-to-evaluate-machine-learning-model-performance-in-python-135b4ae27f7e \n",
    "> - F1 Score: https://towardsdatascience.com/how-to-evaluate-machine-learning-model-performance-in-python-135b4ae27f7e\n",
    "\n",
    "## Evaluation Process\n",
    "> - The submission deadline is on Sunday (9/24) 12PM \\\n",
    "> - Participants are encouraged to submit as much as they can, and we will select the one with the highest F1 score too add to the leaderboard. \n",
    "> - Winners will be announced on Sunday (9/24) 2PM. We'll have a session for winning team to provide a walkthrough of their code, their strategies on how they approached the problem and engage in a discussion.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c227cce",
   "metadata": {},
   "source": [
    "*Back to [top](#2023-Hackathon---Image-Classification-using-CNN)*"
   ]
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
